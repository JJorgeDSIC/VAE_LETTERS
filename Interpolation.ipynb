{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:# Samples: 20000\n",
      "INFO:root:# training samples: 16000\n",
      "INFO:root:# training samples: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2        -0.26666667  0.         -0.13333333 -0.06666667 -0.13333333\n",
      "  0.53333333 -0.33333333  0.13333333 -0.46666667  0.46666667 -0.06666667\n",
      " -0.13333333  0.         -0.26666667 -0.13333333]\n",
      "1.11111111111\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "#logging.basicConfig(filename='output.log',level=logging.DEBUG)\n",
    "#logging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def text2numpy(num, lines_list, dim, labels, l_dict=None, i_dict=None):\n",
    "\n",
    "  \n",
    "    \n",
    "    X = np.zeros((num , dim))\n",
    "    Y = np.zeros((num, labels))\n",
    "\n",
    "    label_list = []\n",
    "\n",
    "    for i,l in enumerate(lines_list[:num]):\n",
    "\n",
    "        tokens=l.strip().split(\",\")\n",
    "        values=tokens[1:]\n",
    "\n",
    "        for d,v in enumerate(values):\n",
    "            X[i,d] = float(v)\n",
    "            \n",
    "        if l_dict != None:\n",
    "            Y[i, l_dict[tokens[0]]] = 1\n",
    "        else:\n",
    "            label_list.append(tokens[0])\n",
    "\n",
    "\n",
    "    if l_dict == None:\n",
    "        \n",
    "        sorted_label_ind_list = [(x,i) for x,i in zip(sorted(list(set(label_list))),range(labels))]\n",
    "        sorted_ind_label_list = [(i,x) for x,i in sorted_label_ind_list]\n",
    "\n",
    "        l_dict = dict(sorted_label_ind_list)\n",
    "        logging.debug(\"Label dict: {}\".format(l_dict))\n",
    "        \n",
    "        i_dict = dict(sorted_ind_label_list)\n",
    "        logging.debug(\"Index dict: {}\".format(i_dict))\n",
    "\n",
    "        for i in range(num_tr_examples):\n",
    "            Y[i, l_dict[label_list[i]]] = 1\n",
    "\n",
    "    \n",
    "    \n",
    "    return X,Y, l_dict, i_dict\n",
    "\n",
    "\n",
    "f = open(\"DATASETS/letters/letter-recognition.data\")\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "logging.debug(\"Input data sample: LABEL, V0, V1, ..., V15\")\n",
    "logging.debug(\"Input labels \\in \\{'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', \\\n",
    "       'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'}\")\n",
    "\n",
    "    \n",
    "\n",
    "total_examples = len(lines)\n",
    "logging.info(\"# Samples: {}\".format(total_examples))\n",
    "\n",
    "num_tr_examples = 16000\n",
    "num_te_examples = total_examples - num_tr_examples\n",
    "dim = 16\n",
    "labels = 26\n",
    "\n",
    "logging.info(\"# training samples: {}\".format(num_tr_examples))\n",
    "logging.info(\"# training samples: {}\".format(num_te_examples))\n",
    "\n",
    "X_tr, Y_tr, label_dict, ind_dict = text2numpy(num_tr_examples, lines, dim, labels)\n",
    "X_te, Y_te, label_dict, ind_dict = text2numpy(num_te_examples, lines[num_tr_examples:],\\\n",
    "                                              dim, labels, label_dict, ind_dict)\n",
    "\n",
    "\n",
    "logging.debug(\"Read examples: {}\".format(X_tr[:10]))\n",
    "\n",
    "for y in Y_tr[:10]:\n",
    "    ind=np.argmax(y, axis=0)\n",
    "    logging.debug(\"Read labels: {}\".format(ind_dict[ind]))\n",
    "\n",
    "for i in lines[:10]:\n",
    "    logging.debug(\"Real instances: {}\".format(i))\n",
    "\n",
    "#Normalization -> From 0-15 to 0-1\n",
    "X_tr = X_tr / 15.0\n",
    "X_te = X_te / 15.0\n",
    "\n",
    "logging.debug(\"Cheking things...{}\".format(np.sum(Y_tr)))\n",
    "logging.debug(\"Cheking things...{}\".format(np.sum(Y_te)))\n",
    "\n",
    "one_sample = X_tr[0,:].copy()\n",
    "another_sample = X_tr[1,:].copy()\n",
    "\n",
    "print((one_sample - another_sample))\n",
    "print(np.sum(np.square(one_sample - another_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Condensed code based on the code from: https://jmetzen.github.io/2015-11-27/vae.html\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def encoder(x, weights, biases):\n",
    "    # Generate probabilistic encoder (recognition network), which\n",
    "    # maps inputs onto a normal distribution in latent space.\n",
    "    # The transformation is parametrized and can be learned.\n",
    "    layer_1 = tf.nn.softplus(tf.add(tf.matmul(x, weights['h1']), biases['b1'])) \n",
    "    layer_2 = tf.nn.softplus(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])) \n",
    "    #Reparametrization trick\n",
    "    z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']), biases['out_mean'])\n",
    "    z_log_sigma_sq = tf.add(tf.matmul(layer_2, weights['out_log_sigma']), biases['out_log_sigma'])\n",
    "    return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "def decoder(z, weights, biases):\n",
    "    # Generate probabilistic decoder (decoder network), which\n",
    "    # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "    # The transformation is parametrized and can be learned.\n",
    "    layer_1 = tf.nn.softplus(tf.add(tf.matmul(z, weights['h1']), biases['b1'])) \n",
    "    layer_2 = tf.nn.softplus(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])) \n",
    "    #x_reconstr_mean = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']), biases['out_mean']))\n",
    "    x_reconstr_mean = (tf.add(tf.matmul(layer_2, weights['out_mean']), biases['out_mean']))\n",
    "    return x_reconstr_mean\n",
    "\n",
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X_tr\n",
    "Y = Y_tr\n",
    "\n",
    "logging.debug(np.mean(X,axis=0))\n",
    "logging.debug(np.mean(X,axis=0).shape)\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "x_dim = X.shape[1]\n",
    "original_shape = x_dim\n",
    "z_dim = 2\n",
    "\n",
    "n_hidden_recog_1 = 128\n",
    "n_hidden_recog_2 = 128\n",
    "\n",
    "n_hidden_gener_1 = 128\n",
    "n_hidden_gener_2 = 128\n",
    "\n",
    "network_weights = dict()\n",
    "network_weights['weights_recog'] = {\n",
    "    'h1': tf.Variable(xavier_init(x_dim, n_hidden_recog_1)),\n",
    "    'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "    'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, z_dim)),\n",
    "    'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, z_dim))}\n",
    "network_weights['biases_recog'] = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "    'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "    'out_mean': tf.Variable(tf.zeros([z_dim], dtype=tf.float32)),\n",
    "    'out_log_sigma': tf.Variable(tf.zeros([z_dim], dtype=tf.float32))}\n",
    "\n",
    "network_weights['weights_gener'] = {\n",
    "    'h1': tf.Variable(xavier_init(z_dim, n_hidden_gener_1)),\n",
    "    'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "    'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, x_dim)),\n",
    "    'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, x_dim))}\n",
    "network_weights['biases_gener'] = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "    'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "    'out_mean': tf.Variable(tf.zeros([x_dim], dtype=tf.float32)),\n",
    "    'out_log_sigma': tf.Variable(tf.zeros([x_dim], dtype=tf.float32))}\n",
    "\n",
    "\n",
    "#Original input\n",
    "x_original = tf.placeholder(tf.float32, [None, x_dim])\n",
    "\n",
    "#Encoding layer\n",
    "z_mean, z_log_sigma_sq = encoder(x_original, network_weights[\"weights_recog\"], network_weights[\"biases_recog\"])\n",
    "\n",
    "eps = tf.random_normal(shape=tf.shape(z_mean), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "\n",
    "#Sampling procedure\n",
    "# z = mu + sigma*epsilon\n",
    "z = tf.add(z_mean, tf.multiply(tf.sqrt(tf.exp(z_log_sigma_sq)), eps))\n",
    "\n",
    "#Decoding layer\n",
    "x_reconstructed = decoder(z, network_weights[\"weights_gener\"], network_weights[\"biases_gener\"])\n",
    "\n",
    "#vector from outside to decode\n",
    "z_input = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "#Decoding layer from outside's vector\n",
    "x_reconstructed_sample = decoder(z_input, network_weights[\"weights_gener\"], network_weights[\"biases_gener\"])\n",
    "\n",
    "#Reconstruction loss per sample (REVIEW)\n",
    "per_sample_reconstructed_loss = tf.reduce_mean(tf.squared_difference(x_original, x_reconstructed), axis=1)\n",
    "\n",
    "reconstructed_loss = tf.reduce_mean(per_sample_reconstructed_loss)\n",
    "\n",
    "kl_divergence_vector = 1. + z_log_sigma_sq - tf.pow(z_mean, 2) - tf.exp(z_log_sigma_sq)\n",
    "\n",
    "latent_loss = -.5 * tf.reduce_sum(kl_divergence_vector, reduction_indices=1)\n",
    "\n",
    "cost = tf.reduce_mean(reconstructed_loss + latent_loss)\n",
    "\n",
    "learning_rate=0.0001\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "MODELS_PATH = \"models_vae/\"\n",
    "\n",
    "training_epochs = 1000\n",
    "display_step = 100\n",
    "batch_size = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "VAE_SAVER = tf.train.Saver()\n",
    "\n",
    "save_path = \"models_vae/VANILLA_VAE.ckpt\"\n",
    "\n",
    "root_dir = \"DATASETS/letters/\"\n",
    "base_file = \"letters_Z_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_z_x = np.load(root_dir + base_file + \"X_training.npy\")\n",
    "dataset_z_y = np.load(root_dir + base_file + \"Y_training.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "std_dev = np.std(dataset_z_x,axis=0)\n",
    "\n",
    "n_samples = dataset_z_x.shape[0]\n",
    "\n",
    "noisy_dataset_z_x = np.zeros((n_samples, z_dim))\n",
    "\n",
    "global_scale = 0.5\n",
    "\n",
    "for i in range(dataset_z_x.shape[0]):\n",
    "    \n",
    "    noise = np.random.normal(loc=0, scale=std_dev)\n",
    "    noisy_dataset_z_x[i] = dataset_z_x[i] + global_scale * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(root_dir + \"noise_\" + base_file + \"X_training.npy\", noisy_dataset_z_x)\n",
    "np.save(root_dir + \"noise_\" + base_file + \"Y_training.npy\", dataset_z_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise - Reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batches: 160\n",
      "Model restored in file: models_vae/VANILLA_VAE.ckpt\n"
     ]
    }
   ],
   "source": [
    "noisy_dataset_x_x = np.zeros((noisy_dataset_z_x.shape[0], original_shape))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "n_batches = noisy_dataset_z_x.shape[0]//batch_size\n",
    "\n",
    "print(\"# batches: {}\".format(n_batches))\n",
    "  \n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    VAE_SAVER.restore(sess, save_path)\n",
    "    print(\"Model restored in file: {}\".format(save_path))\n",
    "\n",
    "    for n_batch in range(n_batches):\n",
    "        \n",
    "        batch_xs = noisy_dataset_z_x[n_batch * batch_size: (n_batch + 1) * batch_size]\n",
    "\n",
    "        rescontructed = sess.run(x_reconstructed_sample,feed_dict={z_input: batch_xs})\n",
    "        \n",
    "        noisy_dataset_x_x[n_batch * batch_size: (n_batch + 1) * batch_size] = rescontructed.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(root_dir + \"noise_reconstructed_\" + base_file + \"X_training.npy\", noisy_dataset_x_x)\n",
    "np.save(root_dir + \"noise_reconstructed_\" + base_file + \"Y_training.npy\", dataset_z_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "\n",
    "n_samples = dataset_z_x.shape[0]\n",
    "\n",
    "n_labels = dataset_z_y.shape[1]\n",
    "\n",
    "#Setting the number of neighbors\n",
    "K = 4\n",
    "\n",
    "indexes_per_class = {}\n",
    "distances_per_class = {}\n",
    "indices_per_class = {}\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for l in range(n_labels):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    indexes_per_class[l] = np.where(dataset_z_y[:,l] == 1)\n",
    "    \n",
    "    #Performing NN over the selected instances (same class)\n",
    "    class_members = dataset_z_x[indexes_per_class[l]]\n",
    "    \n",
    "    #Fitting the subset\n",
    "    nbrs = NearestNeighbors(n_neighbors=K, algorithm='ball_tree').fit(class_members)\n",
    "    \n",
    "    #Getting the distances and indices\n",
    "    distances, indices = nbrs.kneighbors(class_members)\n",
    "    \n",
    "    distances_per_class[l] = distances.copy()\n",
    "    indices_per_class[l] = indices.copy()\n",
    "    \n",
    "    logging.debug(\"Per label time: {}\".format(time.time() - start_time))\n",
    "    \n",
    "logging.debug(\"Total time: {}\".format(time.time() - total_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total number of instances: 192000\n",
      "INFO:root:Total number of instances (pred): 192000\n",
      "INFO:root:Total time: 0.6484637260437012\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "total_interpolated_instances_x = np.zeros((n_samples * K * (K - 1), z_dim))\n",
    "total_interpolated_instances_y = np.zeros((n_samples * K * (K - 1), n_labels))\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for l in range(n_labels):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Picking the class members\n",
    "    class_members = dataset_z_x[indexes_per_class[l]]\n",
    "    \n",
    "    n_members = class_members.shape[0]\n",
    "    \n",
    "    for n_member in range(n_members):\n",
    "        \n",
    "        kneighbors = class_members[indices_per_class[l][n_member]]\n",
    "        \n",
    "        #print(kneighbors)\n",
    "\n",
    "        #Generating interpolations:\n",
    "        #For each pair of neighbouring vectors, a new vector is generated\n",
    "        # K * (K - 1) pairs\n",
    "\n",
    "        degree_of_interpolation = 0.5\n",
    "\n",
    "        for i in range(K):\n",
    "            i_sample = kneighbors[i].copy()\n",
    "            for j in range(K):\n",
    "                if j == i:\n",
    "                    pass\n",
    "                else:\n",
    "                    j_sample = kneighbors[j].copy()\n",
    "\n",
    "                    interpolated_instance = (i_sample - j_sample) * degree_of_interpolation + j_sample\n",
    "\n",
    "                    total_interpolated_instances_x[counter] = interpolated_instance.copy()\n",
    "                    total_interpolated_instances_y[counter, l] = 1\n",
    "                    counter+=1\n",
    "\n",
    "    logging.debug(\"Per label time: {}\".format(time.time() - start_time))\n",
    "\n",
    "logging.info(\"Total number of instances: {}\".format(counter))\n",
    "logging.info(\"Total number of instances (pred): {}\".format(total_interpolated_instances_x.shape[0]))\n",
    "logging.info(\"Total time: {}\".format(time.time() - total_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "perm = np.random.permutation(total_interpolated_instances_x.shape[0])\n",
    "\n",
    "total_interpolated_instances_x = total_interpolated_instances_x[perm]\n",
    "total_interpolated_instances_y = total_interpolated_instances_y[perm]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(root_dir + \"interpolated_\" + base_file + \"X_training.npy\", total_interpolated_instances_x)\n",
    "np.save(root_dir + \"interpolated_\" + base_file + \"Y_training.npy\", total_interpolated_instances_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated - Reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batches: 1920\n",
      "Model restored in file: models_vae/VANILLA_VAE.ckpt\n"
     ]
    }
   ],
   "source": [
    "interpolated_dataset_x_x = np.zeros((total_interpolated_instances_x.shape[0], original_shape))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "n_batches = total_interpolated_instances_x.shape[0]//batch_size\n",
    "\n",
    "print(\"# batches: {}\".format(n_batches))\n",
    "  \n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    VAE_SAVER.restore(sess, save_path)\n",
    "    print(\"Model restored in file: {}\".format(save_path))\n",
    "\n",
    "    for n_batch in range(n_batches):\n",
    "        \n",
    "        batch_xs = total_interpolated_instances_x[n_batch * batch_size: (n_batch + 1) * batch_size]\n",
    "\n",
    "        rescontructed = sess.run(x_reconstructed_sample,feed_dict={z_input: batch_xs})\n",
    "        \n",
    "        interpolated_dataset_x_x[n_batch * batch_size: (n_batch + 1) * batch_size] = rescontructed.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(root_dir + \"interpolated_reconstructed_\" + base_file + \"X_training.npy\", interpolated_dataset_x_x)\n",
    "np.save(root_dir + \"interpolated_reconstructed_\" + base_file + \"Y_training.npy\", total_interpolated_instances_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total time: 0.05968928337097168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "VAE_SAVER = tf.train.Saver()\n",
    "\n",
    "n_samples = dataset_z_x.shape[0]\n",
    "\n",
    "n_labels = dataset_z_y.shape[1]\n",
    "\n",
    "#Setting the number of neighbors\n",
    "K = 4\n",
    "\n",
    "indexes_per_class = {}\n",
    "distances_per_class = {}\n",
    "indices_per_class = {}\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for l in range(n_labels):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    indexes_per_class[l] = np.where(dataset_z_y[:,l] == 1)\n",
    "    \n",
    "    #Performing NN over the selected instances (same class)\n",
    "    class_members = dataset_z_x[indexes_per_class[l]]\n",
    "    \n",
    "    #Fitting the subset\n",
    "    nbrs = NearestNeighbors(n_neighbors=K, algorithm='ball_tree').fit(class_members)\n",
    "    \n",
    "    #Getting the distances and indices\n",
    "    distances, indices = nbrs.kneighbors(class_members)\n",
    "    \n",
    "    distances_per_class[l] = distances.copy()\n",
    "    indices_per_class[l] = indices.copy()\n",
    "    \n",
    "    logging.debug(\"Per label time: {}\".format(time.time() - start_time))\n",
    "    \n",
    "logging.info(\"Total time: {}\".format(time.time() - total_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total number of instances: 192000\n",
      "INFO:root:Total number of instances (pred): 192000\n",
      "INFO:root:Total time: 0.6861999034881592\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "total_extrapolated_instances_x = np.zeros((n_samples * K * (K - 1), z_dim))\n",
    "total_extrapolated_instances_y = np.zeros((n_samples * K * (K - 1), n_labels))\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for l in range(n_labels):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Picking the class members\n",
    "    class_members = dataset_z_x[indexes_per_class[l]]\n",
    "    \n",
    "    n_members = class_members.shape[0]\n",
    "    \n",
    "    for n_member in range(n_members):\n",
    "        \n",
    "        kneighbors = class_members[indices_per_class[l][n_member]]\n",
    "        \n",
    "        #print(kneighbors)\n",
    "\n",
    "        #Generating interpolations:\n",
    "        #For each pair of neighbouring vectors, a new vector is generated\n",
    "        # K * (K - 1) pairs\n",
    "\n",
    "        degree_of_extrapolation = 0.5\n",
    "\n",
    "        for i in range(K):\n",
    "            i_sample = kneighbors[i].copy()\n",
    "            for j in range(K):\n",
    "                if j == i:\n",
    "                    pass\n",
    "                else:\n",
    "                    j_sample = kneighbors[j].copy()\n",
    "\n",
    "                    extrapolated_instance = (j_sample - i_sample) * degree_of_extrapolation + j_sample\n",
    "\n",
    "                    total_extrapolated_instances_x[counter] = extrapolated_instance.copy()\n",
    "                    total_extrapolated_instances_y[counter, l] = 1\n",
    "                    counter+=1\n",
    "\n",
    "    logging.debug(\"Per label time: {}\".format(time.time() - start_time))\n",
    "\n",
    "logging.info(\"Total number of instances: {}\".format(counter))\n",
    "logging.info(\"Total number of instances (pred): {}\".format(total_extrapolated_instances_x.shape[0]))\n",
    "logging.info(\"Total time: {}\".format(time.time() - total_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "perm = np.random.permutation(total_extrapolated_instances_x.shape[0])\n",
    "\n",
    "total_extrapolated_instances_x = total_extrapolated_instances_x[perm]\n",
    "total_extrapolated_instances_y = total_extrapolated_instances_y[perm]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(root_dir + \"extrapolated_\" + base_file + \"X_training.npy\", total_extrapolated_instances_x)\n",
    "np.save(root_dir + \"extrapolated_\" + base_file + \"Y_training.npy\", total_extrapolated_instances_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batches: 1920\n",
      "Model restored in file: models_vae/VANILLA_VAE.ckpt\n"
     ]
    }
   ],
   "source": [
    "extrapolated_dataset_x_x = np.zeros((total_extrapolated_instances_x.shape[0], original_shape))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "n_batches = total_extrapolated_instances_x.shape[0]//batch_size\n",
    "\n",
    "print(\"# batches: {}\".format(n_batches))\n",
    "  \n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    VAE_SAVER.restore(sess, save_path)\n",
    "    print(\"Model restored in file: {}\".format(save_path))\n",
    "\n",
    "    for n_batch in range(n_batches):\n",
    "        \n",
    "        batch_xs = total_extrapolated_instances_x[n_batch * batch_size: (n_batch + 1) * batch_size]\n",
    "\n",
    "        rescontructed = sess.run(x_reconstructed_sample,feed_dict={z_input: batch_xs})\n",
    "        \n",
    "        extrapolated_dataset_x_x[n_batch * batch_size: (n_batch + 1) * batch_size] = rescontructed.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(root_dir + \"extrapolated_reconstructed_\" + base_file + \"X_training.npy\", extrapolated_dataset_x_x)\n",
    "np.save(root_dir + \"extrapolated_reconstructed_\" + base_file + \"Y_training.npy\", total_extrapolated_instances_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
